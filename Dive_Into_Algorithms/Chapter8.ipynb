{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0befb7",
   "metadata": {},
   "source": [
    "## Defining a word list and finding words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "10fcc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The oneperfectly divine thing, the oneglimpse of God's paradisegiven on earth,\" \\\n",
    "       \"is to fight a losingbattle - and notlose it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bc396c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['The','one','perfectly','divine']\n",
    "word_list_copy = [word for word in word_list]\n",
    "has_n = [word for word in word_list if 'n' in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6d60f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'divine']\n"
     ]
    }
   ],
   "source": [
    "print(has_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "299fa120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "746c9f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = list(set([(m.start(), m.end()) for word in word_list for m in re.finditer(word, text)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b80fe188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17, 23), (7, 16), (0, 3), (35, 38), (4, 7)]\n"
     ]
    }
   ],
   "source": [
    "print(locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1af57a",
   "metadata": {},
   "source": [
    "## Checking between existing spaces for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52cfe1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacestarts = [m.start() for m in re.finditer(' ',text)]\n",
    "spacestarts.append(-1)\n",
    "spacestarts.append(len(text))\n",
    "spacestarts.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cd8ddae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 3, 16, 23, 30, 34, 45, 48, 54, 68, 71, 80, 83, 89, 91, 104, 106, 110, 118, 121]\n"
     ]
    }
   ],
   "source": [
    "print(spacestarts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d78541e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacestarts_affline = [ss+1 for ss in spacestarts]\n",
    "spacestarts_affline.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c7905874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 17, 24, 31, 35, 46, 49, 55, 69, 72, 81, 84, 90, 92, 105, 107, 111, 119, 122]\n"
     ]
    }
   ],
   "source": [
    "print(spacestarts_affline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f72f0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "between_spaces = [(spacestarts[k] + 1, spacestarts[k+1]) for k in range(0,len(spacestarts)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "549a32bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3),\n",
       " (4, 16),\n",
       " (17, 23),\n",
       " (24, 30),\n",
       " (31, 34),\n",
       " (35, 45),\n",
       " (46, 48),\n",
       " (49, 54),\n",
       " (55, 68),\n",
       " (69, 71),\n",
       " (72, 80),\n",
       " (81, 83),\n",
       " (84, 89),\n",
       " (90, 91),\n",
       " (92, 104),\n",
       " (105, 106),\n",
       " (107, 110),\n",
       " (111, 118),\n",
       " (119, 121)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "between_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b3dc0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "between_spaces_notvalid = [loc for loc in between_spaces if text[loc[0]:loc[1]] not in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "752d1876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 16), (24, 30), (31, 34), (35, 45), (46, 48), (49, 54), (55, 68), (69, 71), (72, 80), (81, 83), (84, 89), (90, 91), (92, 104), (105, 106), (107, 110), (111, 118), (119, 121)]\n"
     ]
    }
   ],
   "source": [
    "print(between_spaces_notvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09041f",
   "metadata": {},
   "source": [
    "## Using an Imported Corpus to Check for Valid Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5e2e062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a0be2e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\timry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5728d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "wordlist = set(brown.words())\n",
    "word_list = list(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5b566e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [word.replace('*','') for word in word_list]\n",
    "word_list = [word.replace('[','') for word in word_list]\n",
    "word_list = [word.replace(']','') for word in word_list]\n",
    "word_list = [word.replace('?','') for word in word_list]\n",
    "word_list = [word.replace('.','') for word in word_list]\n",
    "word_list = [word.replace('+','') for word in word_list]\n",
    "word_list = [word.replace('/','') for word in word_list]\n",
    "word_list = [word.replace(';','') for word in word_list]\n",
    "word_list = [word.replace(':','') for word in word_list]\n",
    "word_list = [word.replace(',','') for word in word_list]\n",
    "word_list = [word.replace('(','') for word in word_list]\n",
    "word_list = [word.replace(')','') for word in word_list]\n",
    "word_list.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "adf9149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "between_spaces_notvalid = [loc for loc in between_spaces if text[loc[0]:loc[1]] not in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c6c27f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 16), (24, 30), (35, 45), (55, 68), (72, 80), (92, 104), (111, 118)]\n"
     ]
    }
   ],
   "source": [
    "print(between_spaces_notvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a4e843bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_words = [loc for loc in locs if loc[0] in spacestarts_affline and loc[1] not in spacestarts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f90f25d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(35, 38), (4, 7)]\n"
     ]
    }
   ],
   "source": [
    "print(partial_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6c46a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_words_end = [loc for loc in locs if loc[0] not in spacestarts_affline and loc[1] in spacestarts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "40cfdff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 16)]\n"
     ]
    }
   ],
   "source": [
    "print(partial_words_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb98f64",
   "metadata": {},
   "source": [
    "## Finding first and second halves of potential words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b7d492d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = between_spaces_notvalid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b5c579f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "endsofbeginnings = [loc2[1] for loc2 in partial_words if loc2[0] == loc[0] and (loc2[1]-loc[0])>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d375e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "beginningsofends = [loc2[0] for loc2 in partial_words_end if loc2[1] == loc[1] and (loc2[1]-loc[0])>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f9a14065",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = list(set(endsofbeginnings).intersection(beginningsofends))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5c087786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ad63dc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ebaaf197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pivot = np.min(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "04cdecf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "63a14452",
   "metadata": {},
   "outputs": [],
   "source": [
    "textnew = text\n",
    "textnew = textnew.replace(text[loc[0]:loc[1]],text[loc[0]:pivot]+' ' + text[pivot:loc[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "572f20aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The one perfectly divine thing, the oneglimpse of God's paradisegiven on earth,is to fight a losingbattle - and notlose it\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textnew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b5aa4",
   "metadata": {},
   "source": [
    "## def insertspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "312defa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertspaces(text, word_list):\n",
    "    locs = list(set([(m.start(), m.end()) for word in word_list for m in re.finditer(word, text)]))\n",
    "    spacestarts = [m.start() for m in re.finditer(' ',text)]\n",
    "    spacestarts.append(-1)\n",
    "    spacestarts.append(len(text))\n",
    "    spacestarts.sort\n",
    "    spacestarts_affline = [ss+1 for ss in spacestarts]\n",
    "    spacestarts_affline.sort()\n",
    "    partial_words = [loc for loc in locs if loc[0] in spacestarts_affline and loc[1] not in spacestarts]\n",
    "    partial_words_end = [loc for loc in locs if loc[0] not in spacestarts_affline and loc[1] in spacestarts]\n",
    "    between_spaces = [(spacestarts[k] + 1, spacestarts[k+1]) for k in range(0,len(spacestarts)-1)]\n",
    "    between_spaces_notvalid = [loc for loc in between_spaces if text[loc[0]:loc[1]] not in word_list]\n",
    "    textnew = text\n",
    "    for loc in between_spaces_notvalid:\n",
    "        endsofbeginnings = [loc2[1] for loc2 in partial_words if loc2[0] == loc[0] and (loc2[1]-loc[0])>1]\n",
    "        beginningsofends = [loc2[0] for loc2 in partial_words_end if loc2[1] == loc[1] and (loc2[1]-loc[0])>1]\n",
    "        pivot = list(set(endsofbeginnings).intersection(beginningsofends))\n",
    "        if(len(pivot)>0):\n",
    "            pivot = np.min(pivot)\n",
    "            textnew = textnew.replace(text[loc[0]:loc[1]],text[loc[0]:pivot]+' ' + text[pivot:loc[1]])\n",
    "    textnew=textnew.replace('  ',' ')\n",
    "    return(textnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c89540c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The one perfectly divine thing, the one glimpse of God's paradise given on earth,is to fight a losing battle - and not lose it\n"
     ]
    }
   ],
   "source": [
    "print(insertspaces(text,word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd8ea9",
   "metadata": {},
   "source": [
    "## Phrase Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "af0ab21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\timry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloaded this due to an error in the process below per suggestion of error message\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "163c5a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'forks', 'perpetually', 'toward', 'innumerable', 'futures']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = \"Time forks perpetually toward innumerable futures\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fd1835dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "token = nltk.word_tokenize(text)\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "fivegrams = ngrams(token,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2064dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = [ngrams(token,2), ngrams(token,3), ngrams(token, 4), ngrams(token,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3b36463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "file = requests.get('http://www.bradfordtuckfield.com/shakespeare.txt')\n",
    "file = file.text\n",
    "text = file.replace('\\r','')\n",
    "text = text.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4dbc0470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From fairest creatures we desire increase,That thereby beautyÃ¢\\x80\\x99s rose might never die,But as the ri'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b1d50189",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.word_tokenize(text)\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "fivegrams = ngrams(token,5)\n",
    "grams = [ngrams(token,2), ngrams(token,3), ngrams(token, 4), ngrams(token,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3f516df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "search_term = 'life is a'\n",
    "split_term = tuple(search_term.split(' '))\n",
    "search_term_length = len(search_term.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2f006f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counted_grams = Counter(grams[search_term_length-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7d998aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('From', 'fairest', 'creatures', 'we'), 1)\n"
     ]
    }
   ],
   "source": [
    "print(list(counted_grams.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d0f8a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_terms = [element for element in list(counted_grams.items()) if element[0][:-1] == tuple(split_term)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7f2e8f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('life', 'is', 'a', 'tedious'), 1), (('life', 'is', 'a', 'miracle'), 1), (('life', 'is', 'a', 'shuttle'), 1)]\n"
     ]
    }
   ],
   "source": [
    "print(matching_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4875586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(matching_terms)>0):\n",
    "    frequencies = [item[1] for item in matching_terms]\n",
    "    maximum_frequency = np.max(frequencies)\n",
    "    highest_frequency_term = [item[0] for item in matching_terms if item[1] == maximum_frequency][0]\n",
    "    combined_term = ' '.join(highest_frequency_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "be8dc463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('life', 'is', 'a', 'tedious')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_frequency_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "62868ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_suggestion(search_term, text):\n",
    "    token = nltk.word_tokenize(text)\n",
    "    bigrams = ngrams(token,2)\n",
    "    trigrams = ngrams(token,3)\n",
    "    fourgrams = ngrams(token,4)\n",
    "    fivegrams = ngrams(token,5)\n",
    "    grams = [ngrams(token,2), ngrams(token,3), ngrams(token, 4), ngrams(token,5)]    \n",
    "    split_term = tuple(search_term.split(' '))\n",
    "    search_term_length = len(search_term.split(' '))\n",
    "    counted_grams = Counter(grams[search_term_length-1])\n",
    "    combined_term = 'No suggested searches'\n",
    "    matching_terms = [element for element in list(counted_grams.items()) if element[0][:-1] == tuple(split_term)]\n",
    "    if (len(matching_terms)>0):\n",
    "        frequencies = [item[1] for item in matching_terms]\n",
    "        maximum_frequency = np.max(frequencies)\n",
    "        highest_frequency_term = [item[0] for item in matching_terms if item[1] == maximum_frequency][0]\n",
    "        combined_term = ' '.join(highest_frequency_term)\n",
    "    return(combined_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c8595662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life is a tedious\n"
     ]
    }
   ],
   "source": [
    "file = requests.get('http://www.bradfordtuckfield.com/shakespeare.txt')\n",
    "file = file.text\n",
    "text = file.replace('\\r','')\n",
    "text = text.replace('\\n','')\n",
    "print(search_suggestion('life is a', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68eb918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
